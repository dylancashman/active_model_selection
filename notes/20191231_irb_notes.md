# 2019-12-31

After doing a bit of research, I think I'm going to go with /r/baseball instead of /r/nba.  /r/nba is more active, has ~3 million members to baseball's ~1 million, but Idon't need that much scale anyway. NBA is in the middle of the season, full of news, while MLB is in the offseason and there are a lot of posts just with analysis.  I think we'll get a captive audience, since they're bored and missing baseball.  I also think the data is better for MLB.   Apparently there's a site, retrosheet.org, that has game logs going back basically one hundred years, and with good detail going back to the 70s.  I actually want player season data, as well as hall of fame data.  I'm sure I could actually get that from wikipedia, via sparql, but, hopefully someone's done it for me.

This is making me think about the actual experiment.  I think I'll want to let them choose a model over the data, then use it to test out on unseen players.  I can precache all of that stuff, the model predictions, etc.  I'll have to load the models into tensorflow.js or something so that they can predict on the unseen data.  That also means I need to do the data generation in javascript as well.  Yarg.  That's ok, though, that's all within the realm of possibility.

I actually don't know if I need to do a control group, do I?  I can have one group just choose from the models without generating new data points, and then have another group choose using randomly chosen data points, and then a final group choose using the "recommended" data points.  Then, I can test on something like precision, to have them rank the models.  I will need some jitter, so they don't cheat.  I actually probably don't want them to be able to share answers.  Maybe I create like 1000 models, and then subsample 30 of them, and then report precision at top k, as well as resulting improvement above the top ranked model.

Here are our hypotheses:

1. In certain data cases (training doesn't match testing), we'll pick a superior model on average
2. If the data cases, we'll do no worse.

This means I may have to also have a factor which is what kind of data they get.  So I'll probably need a decent amount of people, I should do a power analysis.  This could be a motivation for using this number of people.  I really like this, actually.

Data wise, I found a github repo with a "baseball data bank".  I think it has all the info we would need.


## Email text for reddit modes

-----

Hello r/baseball mods,

I wanted to contact you guys to ask for permission to make a post sharing a research study that I'm working on.  I'm a PhD student in computer science at Tufts University, and I work on tools that help people choose machine learning models.  You can see some of my research at my website (https://www.eecs.tufts.edu/~dcashm01/).  I'm working on an experiment that tests out whether showing the user fake data points helps them choose a model that performs better on unseen data.

To conduct this experiment, I need a participant pool that is knowledgeable about their data, passionate and literate with analytics, and would be motivated to build the best machine learning models possible.  We also need a fairly large participant pool because the effects we are studying may be small.  I immediately thought of r/baseball.

The actual experiment will involve going to a web page hosted at a tufts.edu subdomain.  I would make a post explaining the experiment with a link to that web page.  Participants will fill out a consent form, read some explanations on their task and definitions of key terms, then get brought to the actual web application.  The application will show them a set of machine learning models (10-20) that have been trained on historical MLB data to predict whether a player will be inducted into the hall of fame or not.  The participants will try to rank the models based on how accurate they will be on a list of unseen historical player's careers.  They'll make this decision based on visualizations of the data and the models' predictions.  At the end, we'd give them a score of how good their chosen and ranked models did on held out data, and let them test out the models on those players.  For example, once they chose and ranked the machine learning models, they could see the resulting predictions for Pete Rose or Jorge Posada.  At the very end, they will be asked to fill out some demographic surveys and surveys about their experience with visualizations and analytics.

Before I would release the study, I'd have to get it approved by the Institutional Review Board (IRB) at Tufts.  That process takes a month or so, so if you gave me the go ahead, I would be able to post the experiment some time in February.  We would only track anonymous data (so no reddit usernames, no personally identifiable information).  We would keep track of whether a participant has done the experiment once already, most likely via a cookie that would expire after the experiment is closed (probably about a month).  The data that we gather will hopefully be published in an academic journal and presented at an academic conference.

I hope that this would be a fun and informative exercise for the members of this subreddit in the cold, cold, offseason.  What do you think?

Thanks,

-Dylan